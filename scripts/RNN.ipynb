{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "import glob, os, shutil, sys, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "NUMBER_MFCC = 13           # Number of MFCCs\n",
    "NUMBER_FRAMES = 299        # Numer of Frames\n",
    "NUMBER_TESTSAMPLES = 40    # Number of Testsamples\n",
    "BATCH_SIZE = 5             # Batch Size\n",
    "CLASSES = 7\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samplerate of the input\n",
    "SAMPLERATE = 16000\n",
    "\n",
    "# NFFT - This is the frequency resolution\n",
    "# By default, the FFT size is the first equal or superior power of 2 of the window size.\n",
    "# If we have a samplerate of 16000 Hz and a window size of 32 ms, we get 512 samples in each window.\n",
    "# The next superior power would be 512 so we choose that\n",
    "NFFT = 512\n",
    "\n",
    "# Size of the Window\n",
    "WINDOW_SIZE = 0.032\n",
    "\n",
    "# Window step Size = Window-Duration/8 - Overlapping Parameter\n",
    "WINDOW_STEP = 0.004\n",
    "\n",
    "# Preemph-Filter to reduce noise\n",
    "PREEMPH = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/smu/Desktop/RNN/speakers_short\")\n",
    "\n",
    "for aud in glob.glob(\"*.wav\"):\n",
    "    (rate,sig) = wav.read(aud)\n",
    "    mfcc_feat = mfcc(sig, rate, winlen=WINDOW_SIZE, winstep=WINDOW_STEP, nfft=NFFT)\n",
    "    emotion = \"N\"\n",
    "    if \"W\" in aud:\n",
    "        emotion = \"W\"\n",
    "    elif \"L\" in aud:\n",
    "        emotion = \"L\"\n",
    "    elif \"E\" in aud:\n",
    "        emotion = \"E\"\n",
    "    elif \"A\" in aud:\n",
    "        emotion = \"A\"\n",
    "    elif \"F\" in aud:\n",
    "        emotion = \"F\"\n",
    "    elif \"T\" in aud:\n",
    "        emotion = \"T\"\n",
    "    featurefile = \"../train_data/\" + aud + \"_\" + emotion\n",
    "    np.save(featurefile, mfcc_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/smu/Desktop/RNN/train_data/\"\n",
    "moveto = \"/home/smu/Desktop/RNN/test_data/\"\n",
    "\n",
    "for filename in os.listdir(moveto):\n",
    "    file_path = os.path.join(moveto, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "        \n",
    "for x in range(NUMBER_TESTSAMPLES):\n",
    "    random_file=random.choice(os.listdir(\"/home/smu/Desktop/RNN/train_data\"))\n",
    "    src = path + random_file\n",
    "    dst = moveto + random_file\n",
    "    shutil.move(src,dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorDataset shapes: ((495, 299, 13), (495, 7)), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "data_train_data = []\n",
    "label_train_data = []\n",
    "ltr = []\n",
    "\n",
    "os.chdir(\"/home/smu/Desktop/RNN/train_data\")\n",
    "\n",
    "for txtfile in glob.glob(\"*.npy\"):\n",
    "    \n",
    "    temp = np.load(txtfile)\n",
    "    temp = np.delete(temp, (0), axis=0)\n",
    "    data_train_data.append(temp)\n",
    "        \n",
    " \n",
    "    if \"W\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 0)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(0)\n",
    "    elif \"L\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 1)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(1)\n",
    "    elif \"E\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 2)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(2)\n",
    "    elif \"A\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 3)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(3)\n",
    "    elif \"F\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 4)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(4)\n",
    "    elif \"T\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 5)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(5)\n",
    "    elif \"N\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 6)\n",
    "        label_train_data.append(array)\n",
    "        ltr.append(6)\n",
    "        \n",
    "\n",
    "\n",
    "features_train = tf.convert_to_tensor(data_train_data)\n",
    "labels_train = tf.convert_to_tensor(label_train_data)\n",
    "ltr = tf.convert_to_tensor(ltr)\n",
    "ltr = utils.to_categorical(ltr)\n",
    "\n",
    "labeled_train_data = tf.data.Dataset.from_tensors((features_train, ltr))\n",
    "\n",
    "print(labeled_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorDataset shapes: ((40, 299, 13), (40, 7)), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "data_test_data = []\n",
    "label_test_data = []\n",
    "ltt = []\n",
    "\n",
    "os.chdir(\"/home/smu/Desktop/RNN/test_data\")\n",
    "\n",
    "for txtfile in glob.glob(\"*.npy\"):\n",
    "    \n",
    "    \n",
    "    temp = np.load(txtfile)\n",
    "    temp = np.delete(temp, (0), axis=0)\n",
    "    data_test_data.append(temp)\n",
    "        \n",
    " \n",
    "    if \"W\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 0)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(0)\n",
    "    elif \"L\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 1)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(1)\n",
    "    elif \"E\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 2)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(2)\n",
    "    elif \"A\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 3)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(3)\n",
    "    elif \"F\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 4)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(4)\n",
    "    elif \"T\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 5)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(5)\n",
    "    elif \"N\" in txtfile:\n",
    "        array = np.full(NUMBER_FRAMES, 6)\n",
    "        label_test_data.append(array)\n",
    "        ltt.append(6)\n",
    "        \n",
    "\n",
    "\n",
    "features_test = tf.convert_to_tensor(data_test_data)\n",
    "labels_test = tf.convert_to_tensor(label_test_data)\n",
    "ltt = tf.convert_to_tensor(ltt)\n",
    "ltt = utils.to_categorical(ltt)\n",
    "\n",
    "labeled_test_data = tf.data.Dataset.from_tensors((features_test, ltt))\n",
    "\n",
    "print(labeled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 56,407\n",
      "Trainable params: 56,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM((100), input_shape=(299, 13), return_sequences=False))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "8/8 [==============================] - 2s 269ms/step - loss: 2.0679 - accuracy: 0.1677 - val_loss: 1.9964 - val_accuracy: 0.2250\n",
      "Epoch 2/60\n",
      "8/8 [==============================] - 2s 230ms/step - loss: 1.8772 - accuracy: 0.2384 - val_loss: 1.9389 - val_accuracy: 0.2500\n",
      "Epoch 3/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 1.7725 - accuracy: 0.3131 - val_loss: 1.8977 - val_accuracy: 0.2750\n",
      "Epoch 4/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 1.6844 - accuracy: 0.3556 - val_loss: 1.8682 - val_accuracy: 0.2750\n",
      "Epoch 5/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 1.6243 - accuracy: 0.4141 - val_loss: 1.8433 - val_accuracy: 0.3000\n",
      "Epoch 6/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 1.5654 - accuracy: 0.4283 - val_loss: 1.8307 - val_accuracy: 0.3250\n",
      "Epoch 7/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 1.4968 - accuracy: 0.4687 - val_loss: 1.7991 - val_accuracy: 0.3500\n",
      "Epoch 8/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 1.4345 - accuracy: 0.4626 - val_loss: 1.7759 - val_accuracy: 0.3250\n",
      "Epoch 9/60\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 1.3803 - accuracy: 0.5131 - val_loss: 1.7887 - val_accuracy: 0.2750\n",
      "Epoch 10/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 1.3293 - accuracy: 0.5434 - val_loss: 1.7822 - val_accuracy: 0.2750\n",
      "Epoch 11/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 1.2553 - accuracy: 0.5636 - val_loss: 1.7628 - val_accuracy: 0.2750\n",
      "Epoch 12/60\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 1.2388 - accuracy: 0.5758 - val_loss: 1.7226 - val_accuracy: 0.2750\n",
      "Epoch 13/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 1.1735 - accuracy: 0.5879 - val_loss: 1.7071 - val_accuracy: 0.2750\n",
      "Epoch 14/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 1.1136 - accuracy: 0.6303 - val_loss: 1.7125 - val_accuracy: 0.2750\n",
      "Epoch 15/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 1.0243 - accuracy: 0.6646 - val_loss: 1.6780 - val_accuracy: 0.2750\n",
      "Epoch 16/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.9975 - accuracy: 0.6869 - val_loss: 1.6847 - val_accuracy: 0.3250\n",
      "Epoch 17/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.9658 - accuracy: 0.6727 - val_loss: 1.7108 - val_accuracy: 0.3250\n",
      "Epoch 18/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.9083 - accuracy: 0.7192 - val_loss: 1.7156 - val_accuracy: 0.3250\n",
      "Epoch 19/60\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.8803 - accuracy: 0.7051 - val_loss: 1.7353 - val_accuracy: 0.3000\n",
      "Epoch 20/60\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.8074 - accuracy: 0.7293 - val_loss: 1.7085 - val_accuracy: 0.3250\n",
      "Epoch 21/60\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.7926 - accuracy: 0.7313 - val_loss: 1.7324 - val_accuracy: 0.2750\n",
      "Epoch 22/60\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.7134 - accuracy: 0.7899 - val_loss: 1.7351 - val_accuracy: 0.3000\n",
      "Epoch 23/60\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 0.6754 - accuracy: 0.7798 - val_loss: 1.7679 - val_accuracy: 0.3000\n",
      "Epoch 24/60\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.6843 - accuracy: 0.7677 - val_loss: 1.7719 - val_accuracy: 0.2750\n",
      "Epoch 25/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.6711 - accuracy: 0.7677 - val_loss: 1.7722 - val_accuracy: 0.2750\n",
      "Epoch 26/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.6148 - accuracy: 0.7980 - val_loss: 1.7760 - val_accuracy: 0.2750\n",
      "Epoch 27/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.6335 - accuracy: 0.7758 - val_loss: 1.8564 - val_accuracy: 0.3000\n",
      "Epoch 28/60\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.5874 - accuracy: 0.8182 - val_loss: 1.8641 - val_accuracy: 0.2750\n",
      "Epoch 29/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.5911 - accuracy: 0.7980 - val_loss: 1.8729 - val_accuracy: 0.2500\n",
      "Epoch 30/60\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.4860 - accuracy: 0.8485 - val_loss: 1.8967 - val_accuracy: 0.3000\n",
      "Epoch 31/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 0.5351 - accuracy: 0.8283 - val_loss: 1.9400 - val_accuracy: 0.3000\n",
      "Epoch 32/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.5143 - accuracy: 0.8283 - val_loss: 1.9299 - val_accuracy: 0.2750\n",
      "Epoch 33/60\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.4998 - accuracy: 0.8202 - val_loss: 1.9046 - val_accuracy: 0.2500\n",
      "Epoch 34/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.4124 - accuracy: 0.8646 - val_loss: 1.8830 - val_accuracy: 0.2500\n",
      "Epoch 35/60\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.4341 - accuracy: 0.8667 - val_loss: 1.9168 - val_accuracy: 0.2500\n",
      "Epoch 36/60\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.4267 - accuracy: 0.8606 - val_loss: 1.9983 - val_accuracy: 0.2750\n",
      "Epoch 37/60\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.3782 - accuracy: 0.8848 - val_loss: 2.0565 - val_accuracy: 0.2750\n",
      "Epoch 38/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.3991 - accuracy: 0.8747 - val_loss: 2.1001 - val_accuracy: 0.2750\n",
      "Epoch 39/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.3627 - accuracy: 0.8828 - val_loss: 2.0905 - val_accuracy: 0.2750\n",
      "Epoch 40/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.3399 - accuracy: 0.8970 - val_loss: 2.1576 - val_accuracy: 0.2750\n",
      "Epoch 41/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 0.3268 - accuracy: 0.8848 - val_loss: 2.1444 - val_accuracy: 0.2750\n",
      "Epoch 42/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.3067 - accuracy: 0.9030 - val_loss: 2.1244 - val_accuracy: 0.3000\n",
      "Epoch 43/60\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.3307 - accuracy: 0.8869 - val_loss: 2.1222 - val_accuracy: 0.3000\n",
      "Epoch 44/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.3213 - accuracy: 0.8929 - val_loss: 2.1422 - val_accuracy: 0.3500\n",
      "Epoch 45/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.2739 - accuracy: 0.9152 - val_loss: 2.1793 - val_accuracy: 0.3250\n",
      "Epoch 46/60\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.3261 - accuracy: 0.8889 - val_loss: 2.1087 - val_accuracy: 0.3250\n",
      "Epoch 47/60\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 0.2890 - accuracy: 0.9152 - val_loss: 2.0859 - val_accuracy: 0.3000\n",
      "Epoch 48/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.3054 - accuracy: 0.9010 - val_loss: 2.1488 - val_accuracy: 0.3000\n",
      "Epoch 49/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.2621 - accuracy: 0.9212 - val_loss: 2.2441 - val_accuracy: 0.3000\n",
      "Epoch 50/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.2431 - accuracy: 0.9273 - val_loss: 2.2000 - val_accuracy: 0.2750\n",
      "Epoch 51/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.2331 - accuracy: 0.9273 - val_loss: 2.1263 - val_accuracy: 0.2500\n",
      "Epoch 52/60\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.2871 - accuracy: 0.9051 - val_loss: 2.2156 - val_accuracy: 0.2750\n",
      "Epoch 53/60\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.2331 - accuracy: 0.9232 - val_loss: 2.3876 - val_accuracy: 0.3250\n",
      "Epoch 54/60\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.2360 - accuracy: 0.9152 - val_loss: 2.4119 - val_accuracy: 0.3250\n",
      "Epoch 55/60\n",
      "8/8 [==============================] - 2s 223ms/step - loss: 0.2368 - accuracy: 0.9273 - val_loss: 2.3655 - val_accuracy: 0.3000\n",
      "Epoch 56/60\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.2278 - accuracy: 0.9374 - val_loss: 2.3250 - val_accuracy: 0.3250\n",
      "Epoch 57/60\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.2150 - accuracy: 0.9475 - val_loss: 2.2582 - val_accuracy: 0.2750\n",
      "Epoch 58/60\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 0.2460 - accuracy: 0.9152 - val_loss: 2.2114 - val_accuracy: 0.3250\n",
      "Epoch 59/60\n",
      "8/8 [==============================] - 2s 223ms/step - loss: 0.2255 - accuracy: 0.9333 - val_loss: 2.1841 - val_accuracy: 0.3250\n",
      "Epoch 60/60\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 0.2181 - accuracy: 0.9273 - val_loss: 2.2643 - val_accuracy: 0.3000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(features_train, ltr, epochs=60, batch_size=64, validation_data=(features_test, ltt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "Chosen index:  9\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "\n",
    "# Samplerate of the input\n",
    "SAMPLERATE = 48000\n",
    "\n",
    "# NFFT - This is the frequency resolution\n",
    "# By default, the FFT size is the first equal or superior power of 2 of the window size.\n",
    "# If we have a samplerate of 16000 Hz and a window size of 32 ms, we get 512 samples in each window.\n",
    "# The next superior power would be 512 so we choose that\n",
    "NFFT = 512\n",
    "\n",
    "# Format to read in audio data\n",
    "FORMAT = pyaudio.paInt16\n",
    "\n",
    "# Size of the Window\n",
    "WINDOW_SIZE = 0.032\n",
    "\n",
    "# Window step Size = Window-Duration/8 - Overlapping Parameter\n",
    "WINDOW_STEP = 0.004\n",
    "\n",
    "# Preemph-Filter to reduce noise\n",
    "PREEMPH = 0.97\n",
    "# Input-Device\n",
    "INPUT_DEVICE = \"AT2020 USB: Audio (hw:3,0)\" # Name of the input device\n",
    "\n",
    "# Chunk - Each second we calculate 1/0.004 featuresets, so to get the chunk it has to be samplerate/(1/Windowstep)\n",
    "CHUNK = int(SAMPLERATE/(1/WINDOW_STEP))\n",
    "print(CHUNK)\n",
    "\n",
    "# Record Seconds\n",
    "RECORD_SECONDS = 5\n",
    "\n",
    "# Lookup the index of the desired Input-Device, make sure jack is running\n",
    "pa = pyaudio.PyAudio()\n",
    "chosen_device_index = -1\n",
    "for x in range(0,pa.get_device_count()):\n",
    "    info = pa.get_device_info_by_index(x)\n",
    "    # print (pa.get_device_info_by_index(x))\n",
    "    if info[\"name\"] == INPUT_DEVICE:\n",
    "        chosen_device_index = info[\"index\"]\n",
    "        \n",
    "print (\"Chosen index: \", chosen_device_index)\n",
    "pa.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Wut', 'Langeweile', 'Ekel', 'Angst', 'Freude', 'Trauer', 'Neutral'],\n",
       " [5.077765672467649e-05,\n",
       "  0.0064206672832369804,\n",
       "  0.0005578955169767141,\n",
       "  0.0001853248686529696,\n",
       "  8.640124724479392e-05,\n",
       "  0.9903648495674133,\n",
       "  0.0023340065963566303])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import cycle, islice\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "class DynamicUpdate():\n",
    "    #Suppose we know the x range\n",
    "    min_x = 0.0\n",
    "    max_x = 1.0\n",
    "    SAMPLERATE = 48000\n",
    "    NFFT = 512\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    WINDOW_SIZE = 0.032\n",
    "    WINDOW_STEP = 0.004\n",
    "    PREEMPH = 0.97\n",
    "    RECORD_SECONDS = 5\n",
    "\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.varw = 0.0\n",
    "        self.varl = 0.0\n",
    "        self.vare = 0.0\n",
    "        self.vara = 0.0\n",
    "        self.varf = 0.0\n",
    "        self.vart = 0.0\n",
    "        self.varn = 0.0\n",
    "        self.model = model\n",
    "        self.inputdevice = device\n",
    "        self.CHUNK = int(self.SAMPLERATE/(1/self.WINDOW_STEP))\n",
    "\n",
    "    def on_launch(self, xdata, ydata):\n",
    "        #Set up plot\n",
    "        self.figure, self.ax = plt.subplots()\n",
    "        self.figure.canvas.set_window_title('Emotion Recognition')\n",
    "        self.ax.set_ylim(self.min_x, self.max_x)\n",
    "\n",
    "    def on_running(self, xdata, ydata):\n",
    "        self.ax.bar(xdata, ydata)\n",
    "        self.figure.canvas.draw()\n",
    "        self.figure.canvas.flush_events()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        xdata = ['Wut','Langeweile','Ekel','Angst','Freude','Trauer','Neutral']\n",
    "        ydata = [self.varw,self.varl,self.vare,self.vara,self.varf,self.vart,self.varn]\n",
    "        self.on_launch(xdata, ydata)\n",
    "        self.on_launch(xdata, ydata)\n",
    "        frames = []\n",
    "        result = []\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=self.FORMAT, channels=1, rate=self.SAMPLERATE, input_device_index=self.inputdevice, input=True, frames_per_buffer=self.CHUNK)\n",
    "        for i in range(0, int(self.SAMPLERATE / self.CHUNK * self.RECORD_SECONDS)):\n",
    "            data = stream.read(self.CHUNK, exception_on_overflow = False)\n",
    "            decoded = np.frombuffer(data, 'int16')\n",
    "            mfcc_feat = mfcc(decoded, samplerate=self.SAMPLERATE/3, winlen=self.WINDOW_SIZE, winstep=self.WINDOW_STEP, nfft=self.NFFT)\n",
    "            if len(frames) < 299:\n",
    "                frames.append(mfcc_feat)        \n",
    "            elif len(frames) >= 299:\n",
    "                predict_test = tf.convert_to_tensor(frames)\n",
    "                predict_test = tf.transpose(predict_test, [1, 0, 2])\n",
    "                result = model.predict(predict_test)\n",
    "                ydata = [result.item(0), result.item(1), result.item(2), result.item(3), result.item(4), result.item(5), result.item(6)]\n",
    "                self.on_running(xdata, ydata)\n",
    "                frames = []            \n",
    "        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "d = DynamicUpdate(model, chosen_device_index)\n",
    "d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8485c96fd07d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# d = DynamicUpdate(result.item(0), result.item(1), result.item(2), result.item(3), result.item(4), result.item(5), result.item(6))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rr' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from itertools import cycle, islice\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=FORMAT, channels=1, rate=SAMPLERATE, input_device_index=chosen_device_index, input=True, frames_per_buffer=CHUNK)\n",
    "frames = []\n",
    "result = []\n",
    "\n",
    "for i in range(0, int(SAMPLERATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK, exception_on_overflow = False)\n",
    "    decoded = np.frombuffer(data, 'int16')\n",
    "    mfcc_feat = mfcc(decoded, samplerate=SAMPLERATE/3, winlen=WINDOW_SIZE, winstep=WINDOW_STEP, nfft=NFFT)    \n",
    "    if len(frames) < 299:\n",
    "        frames.append(mfcc_feat)        \n",
    "    elif len(frames) >= 299:\n",
    "        predict_test = tf.convert_to_tensor(frames)\n",
    "        predict_test = tf.transpose(predict_test, [1, 0, 2])\n",
    "        result = model.predict(predict_test)\n",
    "        frames = []\n",
    "        frames.append(mfcc_feat)\n",
    "        print(result)\n",
    "\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "class DynamicUpdate():\n",
    "    #Suppose we know the x range\n",
    "    min_x = 0.0\n",
    "    max_x = 1.0\n",
    "    \n",
    "    def __init__(self, w, l, e, a, f, t, n):\n",
    "        self.varw = w\n",
    "        self.varl = l\n",
    "        self.vare = e\n",
    "        self.vara = a\n",
    "        self.varf = f\n",
    "        self.vart = t\n",
    "        self.varn = n\n",
    "\n",
    "    def on_launch(self, xdata, ydata):\n",
    "        #Set up plot\n",
    "        self.figure, self.ax = plt.subplots()\n",
    "        self.figure.canvas.set_window_title('Emotion Recognition')\n",
    "        self.ax.set_ylim(self.min_x, self.max_x)\n",
    "\n",
    "    def on_running(self, xdata, ydata):\n",
    "        self.ax.bar(xdata, ydata)\n",
    "        self.figure.canvas.draw()\n",
    "        self.figure.canvas.flush_events()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        xdata = ['Wut','Langeweile','Ekel','Angst','Freude','Trauer','Neutral']\n",
    "        ydata = [self.varw,self.varl,self.vare,self.vara,self.varf,self.vart,self.varn]\n",
    "        self.on_launch(xdata, ydata)\n",
    "        self.on_running(xdata, ydata)\n",
    "            \n",
    "\n",
    "        return xdata, ydata\n",
    "\n",
    "# d = DynamicUpdate(result.item(0), result.item(1), result.item(2), result.item(3), result.item(4), result.item(5), result.item(6))\n",
    "d = DynamicUpdate(rr.item(0, 0), rr.item(0, 1), rr.item(0, 2), rr.item(0, 3), rr.item(0, 4), rr.item(0, 5), rr.item(0, 6))\n",
    "d()\n",
    "\n",
    "import operator\n",
    "rw = rr.item(0, 0)\n",
    "rl = rr.item(0, 1)\n",
    "re = rr.item(0, 2)\n",
    "ra = rr.item(0, 3)\n",
    "rf = rr.item(0, 4)\n",
    "rt = rr.item(0, 5)\n",
    "rn = rr.item(0, 6)\n",
    "\n",
    "rlist = []\n",
    "rlist.append(rw)\n",
    "rlist.append(rl)\n",
    "rlist.append(re)\n",
    "rlist.append(ra)\n",
    "rlist.append(rf)\n",
    "rlist.append(rt)\n",
    "rlist.append(rn)\n",
    "\n",
    "max_value = max(rlist)\n",
    "max_index = rlist.index(max_value)\n",
    "\n",
    "if max_index == 0:\n",
    "    print(\"Wut\")\n",
    "elif max_index == 1:\n",
    "    print(\"Langeweile\")\n",
    "elif max_index == 2:\n",
    "    print(\"Ekel\")\n",
    "elif max_index == 3:\n",
    "    print(\"Angst\")\n",
    "elif max_index == 4:\n",
    "    print(\"Freude\")\n",
    "elif max_index == 5:\n",
    "    print(\"Trauer\")\n",
    "elif max_index == 6:\n",
    "    print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
